{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1KXzLGzL--2Ek6OohoVI7dnLAOzk7PoNz",
      "authorship_tag": "ABX9TyPxqI8qg7x1aNJSe6on7O/m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sheryar-bit/RAG-LangChain-Practice-Projects/blob/main/Production_Ready_RAG_ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **To be continued. Till now What I have done**\n",
        "*1- Loaded model: google/gemma-2b-it*\n",
        "\n",
        "*2- extract structured review_text data from raw text using a language model*\n",
        "\n",
        "*3- Made a chain and output parser for structured output*\n",
        "\n",
        "*4- loaded an example .pdf to talk with*\n",
        "\n",
        "**TO DO:**\n",
        "\n",
        "*1- load an entire directory with different files*\n",
        "\n",
        "*2- Splitting the docs*\n",
        "\n",
        "*3- Create Embeddings for RAG system*\n",
        "\n",
        "*4- Set Vector store (will use Chroma)*\n",
        "\n",
        "*5- Perform similarity search*\n",
        "\n",
        "*6- create Retriever*\n",
        "\n",
        "*7- Build RAG chain*\n",
        "\n",
        "*8- Handle follow up QS*\n",
        "\n",
        "*9- Using the History Aware RAG Chain*\n",
        "\n",
        "*10- Built Multi user chatbot using SQLite storage*"
      ],
      "metadata": {
        "id": "3U5KOwwFcc8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install torch transformers langchain faiss-cpu sentence-transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CW4w7Vy8aNt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U langchain-community"
      ],
      "metadata": {
        "collapsed": true,
        "id": "y0vQJxC9anWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5hCW9HMZkTj"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain import HuggingFacePipeline\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install transformers datasets faiss-cpu langchain huggingface_hub pydantic\n"
      ],
      "metadata": {
        "id": "dKDWVKm8ZoHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch"
      ],
      "metadata": {
        "id": "nSwj5LbcZqsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "access_token_read= \"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\"\n",
        "access_token_write=\"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\"\n",
        "login(token=access_token_read)"
      ],
      "metadata": {
        "id": "Ow5xMcVdZvzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain-community\n"
      ],
      "metadata": {
        "id": "7IIXYRQ6vmn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain import HuggingFacePipeline\n",
        "import torch\n",
        "\n",
        "# Model name\n",
        "model_name = \"google/gemma-2b-it\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load causal LM (Phi-3 is decoder-only, not seq2seq)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,   # use half-precision if GPU available\n",
        "    device_map=\"auto\"            # automatically place on GPU/CPU\n",
        ")\n",
        "\n",
        "# Create Hugging Face text generation pipeline\n",
        "hf_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=128,\n",
        "    temperature=0.7,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "# Wrap in LangChain LLM\n",
        "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
        "\n",
        "response = llm.invoke(\"What Natural Language Processing (NLP) is. Explain in detail.\")\n",
        "print(response)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rVeYBsGVZjuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.invoke(\"What Natural Language Processing (NLP) is. Explain in detail.\")\n",
        "print(response)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "aP1Vz2rxY8L9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install typing\n",
        "!pip -q install pydantic"
      ],
      "metadata": {
        "id": "EGJPT3NJdBgg",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel, Field, ValidationError\n",
        "from transformers import pipeline\n",
        "import json\n",
        "\n",
        "# Define structured schema\n",
        "class MobileReview(BaseModel):\n",
        "    phone_model: str = Field(description=\"Name and model of the phone\")\n",
        "    rating: float = Field(description=\"Overall rating out of 5\")\n",
        "    pros: List[str] = Field(description=\"List of positive aspects\")\n",
        "    cons: List[str] = Field(description=\"List of negative aspects\")\n",
        "    summary: str = Field(description=\"Brief summary of the review\")\n",
        "\n",
        "# Raw review text\n",
        "review_text = \"\"\"\n",
        "Just got my hands on the new Galaxy S21 and wow, this thing is slick! The screen is gorgeous,\n",
        "colors pop like crazy. Camera's insane too, especially at night - my Insta game's never been\n",
        "stronger. Battery life's solid, lasts me all day no problem.\n",
        "Not gonna lie though, it's pretty pricey. And what's with ditching the charger? C'mon Samsung.\n",
        "Also, still getting used to the new button layout, keep hitting Bixby by mistake.\n",
        "Overall, I'd say it's a solid 4 out of 5. Great phone, but a few annoying quirks keep it from\n",
        "being perfect. If you're due for an upgrade, definitely worth checking out!\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "response=llm.invoke(review_text)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "0s1VGpTgwHfe",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "#defining the custom prompt\n",
        "prompt=ChatPromptTemplate.from_template(\"Tell me 2 jokes about {foo}\")\n",
        "#define the output pasrser\n",
        "parsers=StrOutputParser()\n",
        "#composing the chain\n",
        "chain = prompt | llm | parsers\n",
        "prompt.invoke({\"foo\": \"programming\"})"
      ],
      "metadata": {
        "id": "3EqrUEtUuuNB",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "141d6498"
      },
      "source": [
        "#composing the chain\n",
        "chain = prompt | llm | parsers\n",
        "#using the chain\n",
        "result= chain.invoke({\"foo\": \"car drivers\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "system_msg= SystemMessage(content= \"You are a helpful assistant that tells jokes.\")\n",
        "human_msg= HumanMessage(content= \"Tell me about programming\")\n",
        "llm.invoke([system_msg, human_msg])"
      ],
      "metadata": {
        "id": "Ov31SJLdkspJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install docx2txt pypdf unstructured"
      ],
      "metadata": {
        "id": "kQU1BpAXlebT",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from typing import List\n",
        "# import os\n",
        "\n",
        "\n",
        "text_splitter= RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    # separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
        "    length_function=len\n",
        ")\n",
        "# docx_loader=Docx2txtLoader(\"/content/Vector_Database.pdf\")\n",
        "\n",
        "pdf_loader = PyPDFLoader(\"/content/Vector_Database.pdf\")\n",
        "documents = pdf_loader.load()\n",
        "splits=text_splitter.split_documents(documents)\n",
        "print(f\"Split the documents into {len(splits)} chunks. \")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jmuv8kW0VImW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits[0]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DT_Kx-F7Z7Iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Sheryar-bit/RAG-LangChain-Practice-Projects.git\n"
      ],
      "metadata": {
        "id": "EAqsb4_1mvOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd RAG-LangChain-Practice-Projects/\n"
      ],
      "metadata": {
        "id": "7-rsobGum5sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add ."
      ],
      "metadata": {
        "id": "svR51VobnNsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"Developing a production ready RAG based Caht bot using Lang chain\""
      ],
      "metadata": {
        "id": "uyD5-UaAnQmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "riiXLZsNndSp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}