{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1KXzLGzL--2Ek6OohoVI7dnLAOzk7PoNz",
      "authorship_tag": "ABX9TyOl/jDc3zLqZSzy7wkzL0s+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sheryar-bit/RAG-LangChain-Practice-Projects/blob/main/Production_Ready_RAG_ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **To be continued. Till now What I have done**\n",
        "*1- Loaded model: google/gemma-2b-it*\n",
        "\n",
        "*2- extract structured review_text data from raw text using a language model*\n",
        "\n",
        "*3- Made a chain and output parser for structured output*\n",
        "\n",
        "*4- loaded an example .pdf to talk with*\n",
        "\n",
        "*5- load an entire directory with different files*\n",
        "\n",
        "*6- Splitting the docs*\n",
        "\n",
        "*7- Create Embeddings for RAG system*\n",
        "\n",
        "*8- Set Vector store (will use Chroma)*\n",
        "\n",
        "*9- Perform similarity search*\n",
        "\n",
        "\n",
        "**TO DO:**\n",
        "\n",
        "*5- Perform similarity search*\n",
        "\n",
        "*6- create Retriever*\n",
        "\n",
        "*7- Build RAG chain*\n",
        "\n",
        "*8- Handle follow up QS*\n",
        "\n",
        "*9- Using the History Aware RAG Chain*\n",
        "\n",
        "*10- Built Multi user chatbot using SQLite storage*"
      ],
      "metadata": {
        "id": "3U5KOwwFcc8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install torch transformers langchain faiss-cpu sentence-transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CW4w7Vy8aNt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U langchain-community"
      ],
      "metadata": {
        "collapsed": true,
        "id": "y0vQJxC9anWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5hCW9HMZkTj"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain import HuggingFacePipeline\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install transformers datasets faiss-cpu langchain huggingface_hub pydantic\n"
      ],
      "metadata": {
        "id": "dKDWVKm8ZoHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch"
      ],
      "metadata": {
        "id": "nSwj5LbcZqsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "access_token_read= \"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\"\n",
        "access_token_write=\"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\"\n",
        "login(token=access_token_read)"
      ],
      "metadata": {
        "id": "Ow5xMcVdZvzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain-community\n"
      ],
      "metadata": {
        "id": "7IIXYRQ6vmn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain import HuggingFacePipeline\n",
        "import torch\n",
        "\n",
        "# Model name\n",
        "model_name = \"google/gemma-2b-it\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load causal LM (Phi-3 is decoder-only, not seq2seq)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,   # use half-precision if GPU available\n",
        "    device_map=\"auto\"            # automatically place on GPU/CPU\n",
        ")\n",
        "\n",
        "# Create Hugging Face text generation pipeline\n",
        "hf_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=128,\n",
        "    temperature=0.7,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "# Wrap in LangChain LLM\n",
        "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
        "\n",
        "response = llm.invoke(\"What Natural Language Processing (NLP) is. Explain in detail.\")\n",
        "print(response)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rVeYBsGVZjuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.invoke(\"What Natural Language Processing (NLP) is. Explain in detail.\")\n",
        "print(response)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "aP1Vz2rxY8L9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install typing\n",
        "!pip -q install pydantic"
      ],
      "metadata": {
        "id": "EGJPT3NJdBgg",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel, Field, ValidationError\n",
        "from transformers import pipeline\n",
        "import json\n",
        "\n",
        "# Define structured schema\n",
        "class MobileReview(BaseModel):\n",
        "    phone_model: str = Field(description=\"Name and model of the phone\")\n",
        "    rating: float = Field(description=\"Overall rating out of 5\")\n",
        "    pros: List[str] = Field(description=\"List of positive aspects\")\n",
        "    cons: List[str] = Field(description=\"List of negative aspects\")\n",
        "    summary: str = Field(description=\"Brief summary of the review\")\n",
        "\n",
        "# Raw review text\n",
        "review_text = \"\"\"\n",
        "Just got my hands on the new Galaxy S21 and wow, this thing is slick! The screen is gorgeous,\n",
        "colors pop like crazy. Camera's insane too, especially at night - my Insta game's never been\n",
        "stronger. Battery life's solid, lasts me all day no problem.\n",
        "Not gonna lie though, it's pretty pricey. And what's with ditching the charger? C'mon Samsung.\n",
        "Also, still getting used to the new button layout, keep hitting Bixby by mistake.\n",
        "Overall, I'd say it's a solid 4 out of 5. Great phone, but a few annoying quirks keep it from\n",
        "being perfect. If you're due for an upgrade, definitely worth checking out!\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "response=llm.invoke(review_text)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "0s1VGpTgwHfe",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "#defining the custom prompt\n",
        "prompt=ChatPromptTemplate.from_template(\"Tell me 2 jokes about {foo}\")\n",
        "#define the output pasrser\n",
        "parsers=StrOutputParser()\n",
        "#composing the chain\n",
        "chain = prompt | llm | parsers\n",
        "prompt.invoke({\"foo\": \"programming\"})"
      ],
      "metadata": {
        "id": "3EqrUEtUuuNB",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "141d6498"
      },
      "source": [
        "#composing the chain\n",
        "chain = prompt | llm | parsers\n",
        "#using the chain\n",
        "result= chain.invoke({\"foo\": \"car drivers\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "system_msg= SystemMessage(content= \"You are a helpful assistant that tells jokes.\")\n",
        "human_msg= HumanMessage(content= \"Tell me about programming\")\n",
        "llm.invoke([system_msg, human_msg])"
      ],
      "metadata": {
        "id": "Ov31SJLdkspJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install docx2txt pypdf unstructured"
      ],
      "metadata": {
        "id": "kQU1BpAXlebT",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from typing import List\n",
        "# import os\n",
        "\n",
        "\n",
        "text_splitter= RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    # separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
        "    length_function=len\n",
        ")\n",
        "# docx_loader=Docx2txtLoader(\"/content/Vector_Database.pdf\")\n",
        "\n",
        "pdf_loader = PyPDFLoader(\"/content/Vector_Database.pdf\")\n",
        "documents = pdf_loader.load()\n",
        "splits=text_splitter.split_documents(documents)\n",
        "print(f\"Split the documents into {len(splits)} chunks. \")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jmuv8kW0VImW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits[0]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DT_Kx-F7Z7Iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Loading Directory*"
      ],
      "metadata": {
        "id": "E6m86Da7iw0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from typing import List\n",
        "from langchain_core.documents import Document\n",
        "import os\n",
        "\n",
        "def load_documents(folder_path: str)->List[Document]:\n",
        "    documents = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        if filename.endswith('.pdf'):\n",
        "            loader = PyPDFLoader(file_path)\n",
        "        elif filename.endswith('.docx'):\n",
        "            loader = Docx2txtLoader(file_path)\n",
        "        else:\n",
        "            print(f\"Unsupported file type: {filename}\")\n",
        "            continue\n",
        "        documents.extend(loader.load())\n",
        "    return documents\n",
        "\n",
        "folder_path = \"/content/sample_data/docs\"\n",
        "documents = load_documents(folder_path)\n",
        "print(f\"Loaded {len(documents)} documents from the folder.\")\n"
      ],
      "metadata": {
        "id": "riiXLZsNndSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install sentence-transformers"
      ],
      "metadata": {
        "id": "_rPupkimjHH5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.document_loaders import DirectoryLoader ,PyPDFLoader\n",
        "\n",
        "# Lightwieght and fast Hugging face model for embedding\n",
        "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
        "\n",
        "# Load all PDFs from a folder\n",
        "loader = DirectoryLoader(\n",
        "    \"/content/sample_data/docs\",      #path\n",
        "    glob=\"*.pdf\",                     # match all thhe PDFs\n",
        "    loader_cls=PyPDFLoader            # use PyPDFLoader for each file .pdf\n",
        ")\n",
        "\n",
        "# load the doc\n",
        "docs = loader.load()\n",
        "\n",
        "print(f\"Loaded {len(docs)} documents\")\n",
        "texts = [d.page_content for d in docs]\n",
        "document_embeddings = embeddings.embed_documents(texts)\n",
        "\n",
        "print(f\"Number of docs: {len(document_embeddings)}\")\n",
        "print(f\"Embedding dimension: {len(document_embeddings[0])}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "8QF1WSIpjIBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain_chroma"
      ],
      "metadata": {
        "id": "mYkGn7FUjL5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "\n",
        "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
        "\n",
        "\n",
        "collection_name= \"my_collection\"\n",
        "\n",
        "vector_store= Chroma.from_documents(\n",
        "    documents=docs,\n",
        "    embedding=embeddings,\n",
        "    collection_name=collection_name\n",
        ")\n",
        "\n",
        "print(\"Vectore store created at './chroma_db'\")"
      ],
      "metadata": {
        "id": "m8JhS0VQjOLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Performing similarity Search*\n"
      ],
      "metadata": {
        "id": "c7a6DvIfjQMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is linear classification?\"\n",
        "search_result=vector_store.similarity_search(query)\n",
        "for i, result in enumerate(search_result, 1):\n",
        "  print(f\"Result {i}:\")\n",
        "  # print(f\"Source {result.metadeta.get('source', 'Unknown')}\")\n",
        "  print(f\"Content {result.page_content}\")\n",
        "  print()"
      ],
      "metadata": {
        "id": "2sP-bJWLjVo4"
      },
      "execution_count": null,
      "outputs": []
},
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 2})\n",
        "retriever_results = retriever.invoke(\"When was GreenGrow Innovations founded?\")\n",
        "print(retriever_results)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ij5O7p69N3O7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain-community"
      ],
      "metadata": {
        "id": "lZ0mATnlIk6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template= \"\"\" Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "prompt=ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "NXrlmx-tIDoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "rag_chain=(\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt #here the RunnablePassthrough() is the question that we pass below could be anything\n",
        "    # | llm\n",
        ")\n",
        "rag_chain.invoke(\"What is vector database?\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WiwJWX8ZIe8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def doc2str(docs):\n",
        "  return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ],
      "metadata": {
        "id": "THA6enU9JgIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "rag_chain=(\n",
        "    {\"context\": retriever | doc2str, \"question\": RunnablePassthrough()} | prompt  | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "rag_chain.invoke(\"What is vector database?\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ps3LJd4_McBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What are vector databases?\"\n",
        "response = rag_chain.invoke(question)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {response}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "X1lPyWiRUgDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conversational RAG**"
      ],
      "metadata": {
        "id": "sAZd_gCyZaTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "chat_history=[]\n",
        "chat_history.extend([\n",
        "    HumanMessage(content=question),\n",
        "    AIMessage(content=response)\n",
        "    ])"
      ],
      "metadata": {
        "id": "KegJlRr7UgsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QlDVKExPaQn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import MessagesPlaceholder, ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "contextualize_q_system_prompt = (\n",
        "    \"Given a chat history and the latest user question \"\n",
        "    \"which might reference context in the chat history, \"\n",
        "    \"formulate a standalone question which can be understood \"\n",
        "    \"without the chat history. Do not answer the question, \"\n",
        "    \"just reformulate it if needed and otherwise return it as it is.\"\n",
        ")\n",
        "\n",
        "contextualize_q_system_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"human\", \"{input}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "contextualize_chain = contextualize_q_system_prompt | llm | StrOutputParser()\n",
        "\n",
        "contextualize_chain.invoke({\n",
        "    \"input\": \"What are embeddings?\",\n",
        "    \"chat_history\": chat_history\n",
        "})\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BnyWBgdiaSR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_history_aware_retriever\n",
        "history_aware_retriever=create_history_aware_retriever(\n",
        "    llm, retriever, contextualize_q_system_prompt\n",
        ")\n",
        "history_aware_retriever.invoke({\n",
        "    \"input\": \"What are vector databases, and when are they used?\",\n",
        "    \"chat_history\": chat_history\n",
        "})\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NHIEgiVadDt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.invoke(\"What are vector databases, and when are they used? \")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YzgyEQnzhkYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm, retriever, contextualize_q_system_prompt\n",
        ")\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI assistant. Use the following context to answer the user's question.\"),\n",
        "    (\"system\", \"Context: {context}\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n"
      ],
      "metadata": {
        "id": "m-JqCMKsnbGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain.invoke({\"input\": \"What are vector databases?\", \"chat_history\": chat_history})"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sQpHA6mmpWMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LSTZ3Cg7plif"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
    }
  ]
}
