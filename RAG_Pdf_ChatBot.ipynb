{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0f1d4394-600d-48f7-acfb-796e679055c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "27c3ef2b-7334-4cdd-9261-7dd50e99e712",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb -q\n",
    "!pip install langchain-community -q\n",
    "!pip install ollama langchain-community -q\n",
    "!pip install fastembed -q\n",
    "!pip install langchain -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "defae8c4-4cd3-4241-a9cc-d74d72c7f210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDoc():\n",
    "    #Loading the doc\n",
    "    loader=PyPDFLoader(\"Vector_Database.pdf\")\n",
    "    pages=loader.load_and_split() #We split the doc since it is log and continous DOC\n",
    "\n",
    "    text_splitter=RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1024,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(pages)\n",
    "    print(f\"Split{len(pages)} documents into {len(chunks)} chunks.\" )\n",
    "    embedding = FastEmbedEmbeddings()\n",
    "    #Cretaing Vetor Stores\n",
    "    vector_store=Chroma.from_documents(documents=chunks, embedding=embedding, persist_directory= \"./chroma_db\")\n",
    "    vector_store.persist()  # Save to disk\n",
    "    print(\"Vector store created and persisted.\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6398e829-8474-472a-ad5d-aeed76140fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split16 documents into 16 chunks.\n",
      "Vector store created and persisted.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x2c5ac0b4190>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getDoc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "701c9638-796b-4864-9941-c20d9e1ff9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "access_token_read= \"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\"\n",
    "access_token_write=\"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\"\n",
    "login(token=access_token_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "38923681-d5f9-41d0-a37d-0853384395f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-huggingface -q\n",
    "!pip install -U langchain-ollama -q\n",
    "# !pip install HuggingFaceHub -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b1689078-245b-4ae0-aa64-127afecb84d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chain():\n",
    "    model = ChatOllama(model=\"qwen2:0.5b\") # a light weight meodel (0.5b), pulled from Ollama\n",
    "    #\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        <s> [Instructions] You are a friendly assistant. Answer the question based only on the following context. \n",
    "        If you don't know the answer, then reply, No Context availabel for this question {input}. [/Instructions] </s> \n",
    "        [Instructions] Question: {input} \n",
    "        Context: {context} \n",
    "        Answer: [/Instructions]\n",
    "        \"\"\"\n",
    "    )\n",
    "    #Load the vector store\n",
    "    embedding = FastEmbedEmbeddings()\n",
    "    vector_store = Chroma( persist_directory=\"./chroma_db\", embedding_function=embedding)\n",
    "\n",
    "    #Create chain\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity_score_threshold\",\n",
    "        search_kwargs=\n",
    "        {   \"k\": 3,\n",
    "            \"score_threshold\": 0.5,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    document_chain = create_stuff_documents_chain(model, prompt)\n",
    "    chain = create_retrieval_chain(retriever, document_chain)\n",
    "    #\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b241b381-6250-41c8-833c-969568f071e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n"
     ]
    }
   ],
   "source": [
    "print(\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "de313d85-36b2-45d4-bc32-3b3d790a7879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query: str):\n",
    "    #\n",
    "    chain = rag_chain()\n",
    "    # invoke chain\n",
    "    result = chain.invoke({\"input\": query})\n",
    "    print(result[\"answer\"])\n",
    "    for doc in result[\"context\"]:\n",
    "        print(\"Source: \", doc.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "627a54b5-bfc3-4e84-b6d6-46c8d594c80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split16 documents into 16 chunks.\n",
      "Vector store created and persisted.\n",
      "Vector databases store a variety of structured data types that can be organized and indexed using a hierarchical structure. They are designed to enable fast searching and retrieval of information based on specific criteria such as keywords or user queries. They typically contain images, text files, audio recordings, video content, complex and unstructured digital objects, among other formats.\n",
      "Source:  Vector_Database.pdf\n",
      "Source:  Vector_Database.pdf\n",
      "Source:  Vector_Database.pdf\n"
     ]
    }
   ],
   "source": [
    "getDoc()\n",
    "ask(\"What is Vector Database?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0df86f3f-040a-4e5f-91df-7441063ca967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split16 documents into 16 chunks.\n",
      "Vector store created and persisted.\n",
      "Vectors Embeddings are learned embeddings that capture the characteristics of data based on an array of numbers. They can have hundreds or thousands of dimensions, and they use specialized models to learn the most relevant information from the data. The embeddings provide a multi-dimensional representation of data, allowing for easier analysis and visualization.\n",
      "Source:  Vector_Database.pdf\n",
      "Source:  Vector_Database.pdf\n",
      "Source:  Vector_Database.pdf\n"
     ]
    }
   ],
   "source": [
    "getDoc()\n",
    "ask(\"what are Vectors Embeddings? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "38f8e44e-1f06-4115-81ae-df9d4ed7536c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split16 documents into 16 chunks.\n",
      "Vector store created and persisted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baba Azam is a well-known Indian politician and writer from the state of Jharkhand, known for his contributions to education in India. He is also associated with various political parties and has been active in politics since 1954. Baba Azam is also a Nobel Prize laureate in Physiology or Medicine.\n"
     ]
    }
   ],
   "source": [
    "getDoc()\n",
    "ask(\"who is baba Azam\") #out of context\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "93320414-77e8-4082-8394-79e0b5acad98",
   "metadata": {},
   "source": [
    "\"hahahahahaah!! wtf is the above response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
